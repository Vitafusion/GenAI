{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "* PEFT technique: LoRA, P-Tuning, Prompt-Tuning\n",
    "* Model: GPT2\n",
    "* Evaluation approach: Hugging face \n",
    "* Fine-tuning dataset: imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa0b37-33fa-43e0-ae4f-bb552d0adaf3",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Data Preprocess and EDA](#data)\n",
    "2. [Loading and Evaluating a Foundation Model](#fundation)\n",
    "3. [Performing Parameter-Efficient Fine-Tuning](#peft)\n",
    "   \n",
    "   3.1 [Peft with LoRA](#LoRA)\n",
    "   \n",
    "   3.2 [Peft with Prompt Tuning](#pttuning)\n",
    "   \n",
    "   3.3 [Peft with P Tuning](#ptuning)\n",
    "   \n",
    "5. [Performing Inference with a PEFT Model](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c338a-d44f-4623-a933-aa4c371cd174",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e41119d1-738f-4f84-9af1-0d3d3df7da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer,GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PromptTuningConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae1879",
   "metadata": {},
   "source": [
    "## Data Preprocess and EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb463e-74d4-4604-8e52-067e38b8e427",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc78f20b-4bf0-4368-8db1-c97562f97003",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Random select 2000 observations as train and test datasets\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(2000))\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e26b2-484e-4f25-8a5d-2f074d65005a",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "185aaaf2-ea4e-4e0c-9d5e-c913cd333217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  There is no relation at all between Fortier an...      1\n",
      "1  This movie is a great. The plot is very true t...      1\n",
      "2  George P. Cosmatos' \"Rambo: First Blood Part I...      0\n",
      "3  In the process of trying to establish the audi...      1\n",
      "4  Yeh, I know -- you're quivering with excitemen...      0\n",
      "                                                text  label\n",
      "0  <br /><br />When I unsuspectedly rented A Thou...      1\n",
      "1  This is the latest entry in the long series of...      1\n",
      "2  This movie was so frustrating. Everything seem...      0\n",
      "3  I was truly and wonderfully surprised at \"O' B...      1\n",
      "4  This movie spends most of its time preaching t...      0\n",
      "Train Data Label Distribution:\n",
      "Percentage of 0: 52.00%\n",
      "Percentage of 1: 48.00%\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "\n",
    "label_counts = train_df['label'].value_counts(normalize=True) * 100\n",
    "print(\"Train Data Label Distribution:\")\n",
    "print(f\"Percentage of 0: {label_counts[0]:.2f}%\")\n",
    "print(f\"Percentage of 1: {label_counts[1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a8eb2-d77b-48c5-97e7-54ae8b4476e5",
   "metadata": {},
   "source": [
    "As we can see from the label distributions. The distribution of data is balanced. Not favor for label 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abe3aa-cb64-4f67-9abe-37ac9c4a752a",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e1f039f-5ab0-4b3f-a272-4fa84d6411e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizer padding and truncation\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Data preprocess\n",
    "train_data = train_data.map(preprocess_function, batched=True)\n",
    "test_data = test_data.map(preprocess_function, batched=True)\n",
    "\n",
    "# Transform data to torch tensor format\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff80d33-bd08-43dd-ac85-bed4666b077e",
   "metadata": {},
   "source": [
    "Here we choose the GPT2 as our fundation model. From the followed results, the training accuracy is around 0.5. Which is close to random guess. Before that, we define our metric and set up training arguments for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load fundation model\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "model.config.pad_token_id=50256\n",
    "\n",
    "# Evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.7916995286941528, 'eval_accuracy': 0.515, 'eval_runtime': 3.7862, 'eval_samples_per_second': 52.824, 'eval_steps_per_second': 13.206}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate fundation model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b3f57a8-a3bd-4af9-9749-1509a948bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./saved_model/original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11882341",
   "metadata": {},
   "source": [
    "### Peft with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe3d44-b9d6-4e20-98aa-38ad7cc1cf0c",
   "metadata": {},
   "source": [
    "LoRA (Low-Rank Adaptation) is a fine tuning method that decompose high rank large parameter metrices into product of lower rank matrices to reduce computation.\n",
    "This method hereby reduces the number of parameters in tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,448 || all params: 124,737,792 || trainable%: 0.2377\n"
     ]
    }
   ],
   "source": [
    "LoRA_config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "LoRA_model = get_peft_model(model, LoRA_config)\n",
    "LoRA_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.789687</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.788505</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.8190373992919922, metrics={'train_runtime': 24.8348, 'train_samples_per_second': 16.106, 'train_steps_per_second': 4.027, 'total_flos': 104882975539200.0, 'train_loss': 0.8190373992919922, 'epoch': 2.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create trainer instance\n",
    "LoRA_trainer = Trainer(\n",
    "    model=LoRA_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "LoRA_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a5c16da-26f2-4579-8011-e1cec6f6f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoRA_model.save_pretrained(\"./saved_model/LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e350c",
   "metadata": {},
   "source": [
    "### Peft with Prompt Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef19e10-124f-4b57-b289-2aacb83288ee",
   "metadata": {},
   "source": [
    "Prompt is a subset of embedding vectors that can lead to better training for language model. Tuning only these vectors is called prompt tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab76286",
   "metadata": {},
   "source": [
    "### Peft with P Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a208c0-e2c1-4ddb-8920-dfff56436ab5",
   "metadata": {},
   "source": [
    "P tuning, or prefix tuning is to tune the prefix vector to get better training performance. It is helpful for allocate large model on device with limited computation resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21852a91-f279-4e88-986c-671e809c4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer,GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PromptTuningConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "859b847e-8c13-47c8-b336-b51721158b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Random select 5000 observations as train and test datasets\n",
    "train_data = dataset['train'].shuffle(seed=41).select(range(2000))\n",
    "test_data = dataset['test'].shuffle(seed=41).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ec584ad-4646-44b7-809f-f0622a0b3c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a5ae80d4034302add08b683e57d72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1abd01774e54dcb95591ed7a7c2f272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizer padding and truncation\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Data preprocess\n",
    "train_data = train_data.map(preprocess_function, batched=True)\n",
    "test_data = test_data.map(preprocess_function, batched=True)\n",
    "\n",
    "# Transform data to torch tensor format\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57d9c7f5-bc4c-466d-b1ec-6a7a5bffe38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.765064537525177, 'eval_accuracy': 0.51, 'eval_runtime': 3.7282, 'eval_samples_per_second': 53.646, 'eval_steps_per_second': 13.411}\n"
     ]
    }
   ],
   "source": [
    "# LoRA \n",
    "LoRA_model = GPT2ForSequenceClassification.from_pretrained(\"./saved_model/LoRA\")\n",
    "LoRA_model.config.pad_token_id=50256\n",
    "LoRA_trainer = Trainer(\n",
    "    model=LoRA_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "LoRA_eval_results = LoRA_trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation results: {LoRA_eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
